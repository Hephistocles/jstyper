%o Draft #1
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[super]{nth}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{import}
\usepackage{lmodern}
\usepackage[sorting=none,backend=bibtex]{biblatex}
\bibliography{mybib}
\raggedbottom
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\newcommand*{\orig}{\ensuremath{\!\multimapinv\!}}

\usepackage{float}
\makeatletter
\newcommand\fs@plainruled{\def\@fs@cfont{\rmfamily}\let\@fs@capt\floatc@plain%
\def\@fs@pre{}%
\def\@fs@mid{\kern2pt\hrule\vspace\abovecaptionskip\relax}%
\def\@fs@post{}%
\let\@fs@iftopcapt\iffalse
}
\makeatother
\floatstyle{plain}

\setlength\partopsep{-\topsep}
\addtolength\partopsep{-\parskip}
\newfloat{program}{thp}{lop}
\floatname{program}{Listing}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable
\begin{document}

\chapter{Introduction}\label{introduction}

JavaScript is a high-level programming language which was originally designed
for little more than validating forms on websites. Today the language is an
essential part of everyday computing, powering demanding online applications
such as image editors, IDEs and online games. The execution context of the
language has also changed, moving from the browser to the server, and now even
being targeted by compilers of other languages. As the scale of JavaScript
applications has increased, so too has the complexity of development projects.
Assurance that a program is bug-free has become more critical, and development
tools have accordingly become more powerful. JavaScript's weak typing
discipline, however, makes it harder to reason about a program's correctness,
and limits the information available to development tools.
% Maybe mention JavaScript's annoying implicit type coercions.

\textit{Types} are a way of grouping together values which support similar
operations. For example, \texttt{number} might be a type representing values
which support addition and subtraction. \texttt{string} might represent a
sequence of characters which doesn't support addition or subtraction, but does
support concatenation. A program cannot subtract a \texttt{number} from a
\texttt{string}, and an attempt to do so is known as a \textit{type error}. We
say that a language is \textit{strongly} typed if such attempts will always
generate some kind of error, whereas a \textit{weakly} typed language may allow
the operation to execute, with sometimes unpredictable consequences. Under
\textit{dynamic} typing, checking for type errors happens at the very last
minute -- during program execution itself. Using a \textit{static} type %chktex 8
checker, the program code can be analysed in advance to identify type errors
before the program ever runs.

Static type checking catches potential bugs earlier in the development cycle,
and can allow programs to run faster, without needing to check types before
every operation. However, it is impossible for a static type checker to
precisely identify all potential type errors, because it cannot know precisely
what the program will do when it runs. If such a type checker existed, it could
be used to determine whether any given program will terminate or not -- which %chktex 8
would violate the undecidability of the halting problem. Instead, a
conservative approximation is made, and some programs which would never
actually exhibit a type error will fail a static type check. This approximation
can be frustrating for developers, who will need to satisfy the type checker by
rewriting their before it will run.

Although languages are often described as being statically or
dynamically-typed, in reality it is compilers and interpreters which enforce a
particular discipline. A compiler for a traditionally statically-typed language
could skip the static type checking phase and insert dynamic type checks
instead. In the other direction, it is sometimes possible to perform static
analysis of a program's code in order to avoid reliance on delayed runtime
checks.

Several such attempts have been made to perform static analysis on JavaScript,
which is typically a dynamically-typed language~\cite{jscript}. ECMAScript
4~\cite{es4}, a proposed extension to the JavaScript standard featured
optional type annotations, but the proposal was never finalised and the
annotations are not present in later versions of the standard.
TypeScript~\cite{ts}, a source-to-source compiler, extends JavaScript syntax
with similar annotations such that all JavaScript programs are valid TypeScript
programs. These approaches have depended on programmer-inserted type
annotations, which limits their utility for legacy codebases. Some projects
have attempted to use Hindley-Milner style analysis to infer the types of
program expressions automatically~\cite{anderson06, tajs}, including the most
recent contribution, Flow~\cite{flow}. 

Any static analysis, however, will struggle to offer reliable results because
of the inherently dynamic nature of the language. Many traditional JavaScript
programming idioms are legitimately inherently untypable. Flow and TypeScript
allow for this using a special `any' type, which will never trigger a static
type error. The obvious disadvantage here is that the static analysis can no
longer make any guarantees about the soundness of the program, as certain parts
of it have effectively not been checked.
% maybe include a JS example here.
% 
%
%
%
%
Additionally, JavaScript code rarely operates in isolation in modern web
applications. Even if a program passes the static type checker, some external
piece of code may introduce a type error by, for example, passing data of the
incorrect type to a well-typed function.

Gradual typing is a technique used to make safety guarantees about code which
integrates both static and dynamic typing. The idea was independently presented
by four sets of authors between September 2006 and January 2007~\cite{gradSiek,
gradTobin, gradMatthews, gradGronski}. Since static analysis cannot guarantee
that the program is free of dynamic type errors, we instead guarantee that only
dynamic properties of the code are able to cause runtime errors -- that the
well-typed code ``can't be blamed''~\cite{cantblame} for any type errors which
arise. This is achieved by protecting the boundaries between the type-safe and
type-unsafe `worlds'. Conversions are made from dynamic types to static types,
and although this conversion process may generate an error (for example trying
to convert a \texttt{number} to a \texttt{string}), the type safety of the
well-typed world is provably preserved.

In this project I have combined the type inference and gradual typing ideas
introduced above to create a type checker and source-to-source compiler for a
subset of JavaScript. The static analysis allows dynamic modification of
object types by assignment to previously undefined properties. Parts of the
source code which are type-safe are statically analysed, allowing safe
interface with untyped code through declared `dynamic' variables.

\chapter{Preparation}\label{preparation} \section{Background}
% Should I cite the lecture notes?
Before development could begin, an understanding of the theoretical
underpinnings of the project was required. Several of the undergraduate courses
offered as part of the Tripos were a valuable introduction, notably the Part IB
course \textit{Semantics of Programming Languages}, and the Part II course
\textit{Types}. \textit{Semantics of Programming Languages} introduced the
proof techniques necessary for proving type soundness, but the example language
used was insufficiently complex to cover all strategies required to prove
safety of a more complete language. Pierce's \textit{Types and Programming
Languages}~\cite{pierce} bridged this gap sufficiently that I was able to look
at recent work defining the operational semantics of real dynamic programming
languages\cite{pythonOpSem}. The \textit{Types} course similarly introduced the
Hindley--Damas--Milner algorithm for type inference, and Pierce's book helped
develop those ideas further. Neither course touched on the topic of gradual
typing, which is very much a topic of current research interest. Fortunately my
supervisor has some expertise in the area, and was able to suggest papers by
Siek~\cite{gradSiek, gradSiek2} and herself~\cite{gradGray} to explore the
area.

In relation to JavaScript itself, I began the project with a strong informal
grasp of the language. The ECMAScript 5 specification~\cite{ecmaSpec} offered a
more rigorous definition, which would be required in defining my own
operational semantics. Carrying out any kind of static analysis on a dynamic
language will result in false negatives, where a safe statement is incorrectly
deemed unsafe. This is preferable to a false positive in terms of guaranteeing
safe operation of the program, but still frustrating for the developer. It can
be important, therefore, to understand the manner in which a language is
actually used. There is little benefit to devising solutions to the problem of
typing dynamic idioms which are hardly ever used, and so I looked through the
results of Richards, Lebresne, Burg and Vitek~\cite{JSBehaviour}, who have
analysed the dynamic behaviour of real JavaScript programs in operation on the
internet.  Finally, it was educational to look at
TypeScript~\cite{ts,understandingTS}, to learn from the decisions the
TypeScript team had made, the solutions they had already found, and to fully
understand the limitations of their solution. I also attended a talk on Flow by
one of the authors, with similar ambitions.

\section{Development Plan}

The project consisted of four discrete tasks: designing a formal specification
to describe the operational semantics and type judgements for my subset,
proving that the subset as specified was sound and implementing both the type
inference system and the gradual typing compiler. Orthogonally to this, the
language was divided into discrete features, such as if-statements, binary
operations and loops. This division meant the project lent itself to an
incremental build model, whereby each build increment introduces a new language
feature to the specification, proof and implementations. This allowed
development to proceed systematically, offering sensible opportunities for
regression testing after each iteration. Fixing bugs highlighted by the
regression testing was also simplified, since only relatively small changes
were introduced in each iteration. This strategy also offered the advantage
that, should my initial ambitions have proven too great, I could cease
development at the end of any iteration and still have a successful system,
albeit supporting fewer features.

Within each iteration, I began with the specification, as a kind of
requirements analysis. The iteration would be considered complete when each of
the other components conformed to the specification. For the two development
tasks, I proceeded in a test-driven manner, whereby I would first write tests
involving the new language feature expecting them to fail. Development would
then be aiming to make these failing test-cases pass. My collection of tests
available for regression testing (and later for benchmarking) thus grew
consistently across the project life-cycle, and by the end of the project 86
such tests had been produced.

Many developers view theoretical computer science as carrying little practical
benefit for them, when the truth is that concrete implementations of the theory
can bring huge practical benefits. It was thus important to me to integrate
smoothly within the existing JavaScript ecosystem rather than enforce a
non-standard workflow for JavaScript developers, such that the barrier to entry
be reduced and the benefits of static typing more easily discoverable.
Languages like OCaml, although commonly used for this kind of project within
the academic community, are unlikely to be present on a web developer's
computer. Indeed, this is a problem Chaudhuri mentioned having for Flow, which
has been released as an open source project but suffers from a lack of
contributors because the project's main users are not able to understand or
make meaningful contributions to the type checker's code. Instead, I decided to
write the type inference system in JavaScript, thus ensuring it could run on
any system. This decision also potentially opened the door for writing a
self-hosting compiler, which could type-check and compile its own source code.
Unfortunately, in practice my implementation relied too much on object
inheritance, which was a feature not planned for implementation.

As an interpreted language, JavaScript is not compiled prior to execution, but
it is often compressed to speed download times to client computers. To aid with
this, a number of static analysis tools do already exist, performing mainly
control-flow analyses to, for example, eliminate unused variables from the
source code (known as \textit{minification}). Although I was not interested in
such analyses, it seemed sensible to integrate with a standard abstract syntax
tree (AST) format, and use existing parsing and code-generation solutions.
UglifyJS~\cite{uglify} is one of the most-used minification libraries for
JavaScript, and has separate components for parsing and code generation but
uses a custom (albeit well-documented) AST format. Esprima~\cite{esprima} is
another JavaScript parser which is marginally faster and uses the SpiderMonkey
AST format standardised by Mozilla. Various analysis tools and code generators
exist for this format, but I decided that the UglifyJS format was actually more
useful for my purposes, and parse speeds was not a priority. For example, the
SpiderMonkey AST has a single representation of any unary expression, when in
practice it can be useful to distinguish between prefix and suffix expressions.
Additionally, UglifyJS uses object inheritance to define its node types, where
Esprima prefers plain objects. Inheritance made it straightforward to define
type judgements once for multiple related nodes, though retrospectively it is a
shame that this may have prevented my compiler from being self-hosting.

\chapter{Implementation}\label{implementation}

\section{Specification}
The specification of my JavaScript subset has two components -- a definition of
the operational semantics, and a definition of the typing judgement used. These
definitions are used for two halves of a proof of \textit{type soundness} -- a
proof of \textit{progress}, and a proof of type \textit{preservation}.
Evaluation of a program is modelled as a series of transitions from one
configuration to another. The proof of progress ensures that, if the program is
deemed `well-typed', there must be a transition available unless the program
has in fact reduced down to a single value representing the result of the
computation -- so the program will never get stuck in a well-typed
configuration. The proof of type preservation roughly shows that, if a
statement $m$ is deemed well-typed and is reducible to some expression $m'$,
then $m'$ must also be well-typed. The result of this is that, as long as we
can verify that the program is well-typed to start off with, we know that all
subsequent configurations must be well-typed and hence that there must be
further transitions available. The verification of the program in the first
place is what the implementation of the type inference system achieves.

The operational semantics takes the form of about 60 inductive rules describing
all possible transitions from one step of computation to another. A step of
computation is described by a configuration triple of the form $$\langle m,
s,\theta\rangle$$

$m$ is the program code remaining to be executed, $s$ represents the current
scope (the set of variables accessible at this program point); and $\theta$
represents the heap, where variable values are stored.  Concretely, $s$ is a
function from variable identifiers to heap addresses, and $\theta$ is a
function from heap addresses to values -- which may either be primitive values
(such as numbers or function closures), or themselves be functions from
property names to values (in the case of objects and arrays).  Many operational
semantics do not distinguish between the scope and the heap, combining both
into a single entity normally known as the store. The separation is needed
here, however, because JavaScript is not a \textit{pure} language.  When a
function is called, it may have side-effects which must be recorded, but which
should not affect the calling scope (listing \ref{lst:sideEffects}).
\begin{program}[h]
  \begin{minted}[numbersep=-12pt,linenos]{javascript}
	function makeCounter(n) {
	  var i = n;
	  return function() {
		if (i>0) {
		  i--;
		}
		return i;
	  }
	}

	var count = makeCounter(10);
	count(); // i is now 9, but the current scope is unchanged
  \end{minted}
  \label{lst:sideEffects}
  \caption{A function call with side effects}
\end{program}
% Example code having side effects (probably a counter)
The only way to achieve this is to have a separate heap (which is modified by
the function call) and scope (which is not).

% include transition rule Seq2
A transition rule is described by a series of premises, which, if true,
indicate that the conclusion must also be true. For example, the rule
\textsc{Seq2} tells us that, if some statement $m_1$ can reduce to $m_1'$ using
the store $(s,\theta)$, then it must also be possible to reduce any sequence of
statements beginning with $m_1$ with the same effects on the store.

The second component of the specification is the typing judgement, which is
divided into typing judgements for expressions (which have a type) and
typability judgements for statements. A typing judgement has the form 
$$\Gamma\vdash e : T\ |_C\ \Gamma'$$

$\Gamma$ represents the type store, functioning similarly to the store from our
operational semantics by providing a map from variable identifiers to types.
$e$ is the expression being judged, and $T$ is a valid type for this
expression. $C$ is a set of constraints which must be satisfied for the type
judgement to be considered valid. Constraints will either be straight equality
($T = number$) or subtype constraints ($T_1 \succeq T_2$). $T_2$ is a valid
subtype of $T_1$ if both types are object types, and if every property of $T_1$
is also present in $T_2$. For example, a \texttt{shape} type may have
properties for \texttt{area} and \texttt{colour}. Any other type which has both
\texttt{area} and \texttt{colour} may be a valid subtype of \texttt{shape},
regardless of which other properties it has. Finally, evaluating the expression
may result in changes to the type store (for example introducing a new
variable), and these changes are reflected in $\Gamma'$, which will be used for
judging the type of subsequent expressions. Statements $m$ do not themselves
have a type, so a type judgement would not make sense. Instead, we use a
`typability' judgement to assert that all sub-expressions of the statement are
well-typed.  This judgement has the same form as the typing judgements, without
$T$: 
$$\Gamma \vdash m\ |_C\ \Gamma'$$

The structure of the inductive rules is the same as for the operational
semantics. For example, rule \textsc{PropType} indicates that \texttt{e.l} only
has a valid type if \texttt{e} does, and if the type of \texttt{e} is a subtype
of the type $\{l: T\}$ (i.e.~if the type of \texttt{e} has at least the property
\texttt{l}).

Most of the judgements follow reasonably closely an intuitive understanding of
the operation of JavaScript, but a few merit closer attention. Legal assignment
targets are not limited in the ECMAScript specification by syntax, but rather
by whether or not expressions resolve to a reference. For my specification, I
have simplified this in order to make a decision about whether an assignment is
valid on the basis of syntax alone. I have done this by specifying that the
only valid left hand side of an assignment is a value reference (\textit{vRef})
or an expression which will reduce down to a value reference (an
\textit{assignTarget}). A value reference represents a value stored on the
heap. A variable identifier \texttt{x}, for example, is a valid value
reference. The value on the heap may be an object or array pointing to other
values, and so \texttt{x.l} and \texttt{x[n]} are value references. Although
value references share a syntactical similarity to dereferencable expressions,
they are distinct and do not reduce down to the value itself. This is
important, else we could end up with assignments of the form \texttt{5 = 6;}
which make little sense. This simplification has the consequence of deeming
some valid JavaScript assignments illegal, such as the following expression:
\begin{program}[H]
  \begin{minted}{js}
	(function(){ return {}; }).x = 5;
  \end{minted}
\end{program}
Although this is a valid JavaScript assignment, it would be difficult to use in
our operational semantics, since the object returned by the function has no
identifier, and hence no entry in either $s$ or $\Gamma$. In contrast, there is
a direct relation between a value reference and a location in the store,
defined by the function $addr(vRef, s, \theta)$.

One area where I deliberately deviate from the ECMAScript specification is in
disallowing assignments of different types. If a variable \texttt{x} contains a
number, it should not be possible to assign a string to it, as this makes it
unclear whether \texttt{x} should be considered a string or a number
henceforth, especially without some sort of control flow analysis to
disambiguate cases where it is not clear whether or not the assignment will
take place. In practice this is not a great problem in most cases, since most
programmers will intuitively follow a kind of static typing discipline
themselves, even when programming in a dynamic language. It's rarely the case
that a single variable is used for both a string and a number.
% This is something Avik mentioned for flow, would be good to get some stats

The exception to this is the case of objects. It is very common to add
properties to an object after its initialisation. Some approaches to typing
JavaScript make the assumption that all objects have a larger `true' type than
they are initialised with, with uninitialised properties known as
`\textit{potential}' types which will be added later during some initialisation
phase~\cite{anderson05}. Analysis of JavaScript's dynamic behaviour in
practice, however, suggests that adding properties is likely to happen at any
point in the object's life cycle~\cite{JSBehaviour}, and so potential types are
not a good model for dynamic objects.

\begin{program}[h]
  \begin{minted}[numbersep=-12pt,linenos]{js}
	function f(x) {
	  x.foo = 5;
	  return x.bar;
	}
  \end{minted}
  \caption{property addition}\label{lst:propAdd}
\end{program}

Instead, one must consider two object types -- one where the new property is
present, and one where it is not. Listing~\ref{lst:propAdd} illustrates one
situation in which two distinct types are needed. At the start of the function,
\texttt{x} must have type $\mathtt{\{bar:T\}}$, otherwise line 3 will involve
an undefined property access. At the end of the function, \texttt{x} should
have type $\mathtt{\{foo:number, bar:T\}}$ after adding the property
\texttt{foo}.  When determining the type of \texttt{f}, we must have access to
both of these types in order to construct the correct function type:
$\mathtt{\{bar:T\} \rightarrow \{foo:number, bar:T\}}$. Let the initial type of
\texttt{x} be $T_x$, and let the type of \texttt{x} after the property addition
be $T_x'$. It would be nice to simply replace $T_x$ with $T_x'$ in the type
store after the property addition, but sadly this is insufficient. The reason
for this is the access to the property \texttt{bar} on line 3.  This is only a
valid operation if \texttt{x} is of a type which has a property \texttt{bar},
and indeed the type judgement for this statement will generate a constraint of
the form 
\begin{equation}
  \label{eq:barC}
  \{bar:T\} \succeq T_x'.
\end{equation}

The problem is that the constraint is on $T_x'$, and not on $T_x$. $T_x$ is
left essentially unconstrained, and \texttt{f} is incorrectly given the type
$\mathtt{(\{\} \rightarrow \mathtt{\{foo:number, bar:t\}})}$. In order to
correctly judge the type of this function, we need constraint~\eqref{eq:barC}
to pass through $T_x'$ to $T_x$. I do this by attaching $T_x$ as the
\textit{origin} of $T_x'$. I represent this by $T_x'\orig T_x$.  When a
non-root object type (i.e.~one which has an origin defined) is constrained as a
subtype, it may either have the required properties itself, or find them in the
origin. Formally,
\begin{equation}
  \begin{split}
	T_1\enspace \succeq\enspace T_2\orig T_2' \iff  \exists\ & T_a, T_b\ .\\
	& T_a\cup T_b = T_1 \quad\land  \\
	& \forall \left\{l: T\right\} \in T_a \enspace.\enspace (\{l: T'\}\in T_2\enspace \land \enspace T \succeq T') \quad\land \\
	& T_b \succeq T_2'
  \end{split}
\end{equation}
and
\begin{equation}
  T_1\orig T_1'\enspace \succeq \enspace T_2 \iff T_1\succeq T_2 \land T_1' \succeq T_2
\end{equation}

\begin{figure}
  % TODO: add example code listing which would result in this structure
  \begin{center}
  	\resizebox{3in}{!}{
  	  \def\svgwidth{200pt}
  	  \import{../res/}{origin.pdf_tex}
  	}
  \end{center}
  \caption{Chain of origin objects}
  \label{fig:origin}
\end{figure}
This system allows the constraint to propagate along the origin chain, finally
enforcing it on the root object. \textsc{PropType} describes this for any chain of
object value references. Note that a a full object chain is created (as
required for $T_x'$), not simply an object for the new member, and that each
element of this chain is needs to be attached to its origin so that constraints
at other levels are correctly passed through. \textsc{PropType} does not allow adding
properties to value references which include array types, because adding a
property to one object in an array will not change any of the other objects,
and so the type of the array itself cannot be modified in this way.

These origin chains are largely orthogonal to the rest of the typing rules,
except for those involving interesting control flow (i.e.~rules for
\texttt{if}, \texttt{while} and \texttt{for}). This is due to the possibility
of a branch not being taken, and so we cannot be certain whether or not a
certain property has been added. Instead, we merge together the different
potential type environments. 

The final interesting part of the specification is function closures.
JavaScript uses a lexical rather than dynamic scope, meaning that the scope
available during a function call is that surrounding the function definition
rather than the call site. In order to model this, transition rule
\textsc{Func} reduces a function into a \textit{function closure} which
captures both the function itself and the current scope. When this function is
eventually called (e.g.~ in rule \textsc{CallNamed}), this scope is used for
further reductions (as in rule \textsc{CallBody1}). An analogous process
happens in the typing rules, capturing the type environment in a closure type.
Note in particular that we assert that, when using the closure's type
environment, we must first assert that the captured store is well-typed under
this type environment ($\gamma \vdash (s, \theta)$). This is necessary for our
proof of progress, discussed in the next section.

\section{Proof}

\section{Inference Implementation}
Recursive `this' \\
Solving constraints iteratively (rather than in one pass at the end) \\
Solving substructure constraints (several times\ldots) \\

\section{Gradual Typing}
Explanation as explicit casts + why this allows proofs of safety \\
Guards and mimics for higher-order objects \\
distinction between `dynamic' and `import' to reduce overhead \\

\printbibliography{}

\chapter{Evaluation}\label{evaluation}
\section{`Sanity Check': visual inspection of my test cases}
\section{Correctness tests comparing output of code with and without gradual typing}
\section{Speed and Memory Overhead}


\chapter{Conclusions}
\section{Future work}
Prototypal Inheritance, Strings as objects, functions as objects \\
Union types, with discharge under type-checks
Auto-detect imported vars by scope analysis \\
Fine-grained object modification by control flow analysis

\end{document}

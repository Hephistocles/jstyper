% Draft #2
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}  % turns references into hyperlinks
\usepackage[margin=25mm]{geometry} % adjusts page layout
\usepackage{graphicx} % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}  % only needed to allow inclusion of proposal.tex
\usepackage[super]{nth}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{txfonts}
\usepackage{import}
\usepackage{lmodern}
\usepackage[sorting=none,backend=bibtex]{biblatex}
\usepackage{proof}
\bibliography{mybib}
\raggedbottom
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\usemintedstyle{friendly}
\newcommand*{\js}{\mintinline{javascript}}
\newcommand*{\orig}{\ensuremath{\!\multimapinv\!}}

\usepackage{float}
\makeatletter
\newcommand\fs@plainruled{\def\@fs@cfont{\rmfamily}\let\@fs@capt\floatc@plain%
\def\@fs@pre{}%
\def\@fs@mid{\kern2pt\hrule\vspace\abovecaptionskip\relax}%
\def\@fs@post{}%
\let\@fs@iftopcapt\iffalse
}
\makeatother
\floatstyle{plain}

\setlength\partopsep{-\topsep}
\addtolength\partopsep{-\parskip}
\newfloat{program}{thp}{lop}
\floatname{program}{Listing}

\renewcommand{\baselinestretch}{1.1}  % adjust line spacing to make
                    % more readable
\begin{document}

\chapter{Introduction}\label{introduction}

Types are a way of grouping together values which support similar operations.
For example, \texttt{number} might be a type representing values which support
addition and subtraction. \texttt{string} might represent a sequence of
characters which supports concatenation, but doesn't support addition or
subtraction. A program in a language with these types cannot subtract a
\texttt{number} from a \texttt{string}, and an attempt to do so is known as a
\textit{type error}. We say that a language is \textit{strongly} typed if such
attempts will always generate some kind of error, whereas a \textit{weakly}
typed language may allow the operation to execute, with sometimes
unpredictable consequences. Under \textit{dynamic} typing, checking for type
errors happens at the very last minute -- during program execution itself.
Using a \textit{static} type checker, the program code can be analysed in
advance to identify type errors before the program runs.

JavaScript is a high-level, dynamically-typed programming language which was
originally designed for little more than validating forms on websites. Today
the language is an essential part of everyday computing, powering demanding
online applications such as image editors, IDEs and online games. The
execution context of the language has also changed, moving from the browser to
the server, and now even being targeted by other languages'
compilers~\cite{asm}. As the scale of JavaScript applications has increased,
so too has the complexity of development projects. Assurance that a program is
bug-free has become more critical, both for online companies which rely on
JavaScript for e-commerce, and for the people that use their applications. A
security bug in an online banking application, for example, could be
devastating. JavaScript's weak typing discipline, however, makes it hard to
reason about a program's correctness, and limits the information available to
development tools. This problem is exacerbated by JavaScript's type
\textit{coercion}, which permits expressions which would normally be deemed
untypable by, for example, converting any object to the value \js{true} when
placed in a context expecting a boolean. This can result in silent propagation
of type errors, providing no errors to the developer.

Static type checking catches potential bugs earlier in the development cycle,
and can allow programs to run faster, without needing to check types before
every operation. However, it is impossible for a static type checker to
precisely identify all potential type errors, because it cannot know precisely
what the program will do when it runs. If such a type checker existed, it could
be used to determine whether any given program will terminate or not -- which %chktex 8
would violate the undecidability of the halting problem. Instead, a
conservative approximation is made, and some programs which would never
actually exhibit a type error will fail a static type check. This approximation
can be frustrating for developers, who will need to satisfy the type checker by
rewriting their code before it will run.

Although languages are often described as being statically or
dynamically-typed, in reality it is compilers and interpreters which enforce a
particular discipline. A compiler for a traditionally statically-typed language
could skip the static type checking phase and insert dynamic type checks
instead. In the other direction, it is sometimes possible to perform static
analysis of a program's code in order to avoid reliance on delayed runtime
checks.

Several such attempts have been made to perform static analysis on JavaScript,
which is typically a dynamically-typed language. ECMAScript~4~\cite{es4},
a proposed extension to the JavaScript standard, featured optional type
annotations, but the proposal was never finalised and the annotations are not
present in later versions of the standard. TypeScript~\cite{ts}, uses a
superset of JavaScript syntax with similar annotations, and compiles this back
into JavaScript. These approaches have depended on programmer-inserted type
annotations, which limit their utility for legacy codebases. Some projects
have attempted to use Hindley-Milner style analysis to infer the types of
program expressions automatically~\cite{anderson06, tajs, guha}, including the most
recent contribution, Flow~\cite{flow}. 

Any static analysis, however, will struggle to offer reliable results because
of the inherently dynamic nature of the language. Many traditional JavaScript
programming idioms are legitimately untypable. JavaScript has no
mechanism for overloading functions, for example, so many developers write
functions expecting different parameter types from call-to-call, and use manual
type checks to handle them (see listing~\ref{lst:dynamicIdiom}). Flow and
TypeScript allow for untypable functions using a special `any' type, which will
never trigger a static type error. The obvious disadvantage here is that the
static analysis can no longer make any guarantees about the soundness of the
program, as certain parts of it have effectively not been checked.
\begin{program}[t]
 \begin{minted}[linenos,numbersep=-12pt]{javascript}
 	// adapted from the Prototype library
 	function serialize(val) {
	 	switch (typeof val) {
		 	case "boolean":
			 	return val ? "true" : "false";
		 	case "number":
			 	return "" + val;
		 	case "string":
			 	return val;
	 	}
 	}	
 \end{minted}
 \label{lst:dynamicIdiom}
 \caption{Dynamic idioms in JavaScript}
\end{program}
Additionally, JavaScript code rarely operates in isolation in modern web
applications. Even if a program passes the static type checker, some external
piece of code may introduce a type error by, for example, passing data of the
incorrect type to a well-typed function.

Gradual typing is a technique used to make safety guarantees about code which
combines both static and dynamic typing. Initially looking at interoperation
between static and dynamic languages~\cite{gray2005fine}, the concepts are
also applicable here, where the division between static and dynamic code lies
within one language rather than between several. Since static analysis cannot
guarantee that the program is free of dynamic type errors, we instead
guarantee that only dynamic properties of the code are able to cause runtime
errors -- that the well-typed code ``can't be blamed"~\cite{cantblame} for
any type errors which arise. This is achieved by protecting the boundaries
between the type-safe and type- unsafe `worlds'. Conversions are made from
dynamic types to static types, and although this conversion process may
generate an error (for example trying to convert a \texttt{number} to a
\texttt{string}), the type safety of the well- typed world is provably
preserved.

In this project, I have defined a formal operational semantics and a type
system for a subset of JavaScript, and created a proof that the system defined
has the properties of \textit{progress} and \textit{type preservation}. I have
successfully implemented type inference according to this specification in
order to create a static type checker, and this implementation has been checked
by a large suite of tests. In doing this, I realised that the subset of
JavaScript initially planned to be supported was insufficient for `real-world'
code, and added arrays and object growth to both the specification and
implementation. This enabled me to test my implementation using seven tests
from the Mozilla SunSpider JavaScript benchmark suite. The only extension task
left incomplete is the inclusion of object inheritance via object prototypes,
but I present a starting point to this, which could fruitfully be built upon in
future work.

\chapter{Preparation}\label{preparation} \section{Background}

Before development could begin, an understanding of the theoretical
underpinnings of the project was required. Several of the undergraduate courses
offered as part of the Tripos were a valuable introduction, notably the Part IB
course \textit{Semantics of Programming Languages}, and the Part II courses
\textit{Types} and \textit{Optimising Compilers}. \textit{Semantics of
Programming Languages} introduced the proof techniques necessary for proving
type soundness, but for an insufficiently complex example language to cover all
strategies required. Pierce's \textit{Types and Programming
Languages}~\cite{pierce} bridged this gap sufficiently that I was able to look
at recent work defining the operational semantics of real dynamic programming
languages~\cite{pythonOpSem}. The \textit{Types} course similarly introduced
the Hindley--Damas--Milner algorithm for type inference, and Pierce's book
helped develop those ideas further. Neither course touched on the topic of
gradual typing, which is very much a topic of current research interest. A few
papers by Siek~\cite{gradSiek, gradSiek2} and Gray~\cite{gradGray} proved
sufficiently accessible, however, for me to explore the area.
\textit{Optimising Compilers} presented techniques for control flow analysis,
which usefully complement the inference-based analyses, for example by checking
that a variable has been defined before use.

In relation to JavaScript itself, I began the project with a strong informal
grasp of the language. The ECMAScript 5 specification~\cite{ecmaSpec} offered a
more rigorous definition, which would be required in defining my own
operational semantics. Some work has already been done attempting to formalise
the semantics of JavaScript~\cite{guha2010essence}, providing useful inspiration in designing my own
operational semantics. Carrying out any kind of static analysis on a dynamic
language will result in false negatives, where a safe statement is incorrectly
deemed unsafe. This is preferable to a false positive in terms of guaranteeing
safe operation of the program, but still frustrating for the developer. It can
be important, therefore, to understand the manner in which a language is
actually used. There is little benefit in devising solutions to the problem of
typing dynamic idioms which are hardly ever used, and so I looked through the
results of Richards, Lebresne, Burg and Vitek~\cite{JSBehaviour}, who have
analysed the dynamic behaviour of real JavaScript programs in operation on the
internet. Finally, it was educational to look at
TypeScript~\cite{ts,understandingTS}, to learn from the decisions the
TypeScript team had made, the solutions they had already found, and to fully
understand the limitations of their solution. I also attended a talk on Flow by
one of the authors, with similar ambitions.

\section{Development Plan}

The project consisted of four discrete tasks: designing a formal specification
to describe the operational semantics and type judgements for my subset,
proving that the subset as specified was sound and implementing both the type
inference system and the gradual typing compiler. Orthogonally to this, the
language was divided into discrete features, such as if-statements, binary
operations and loops. This division meant the project lent itself to an
incremental build model, whereby each build increment introduced a new language
feature to the specification, proof and implementations. This allowed
development to proceed systematically, offering sensible opportunities for
regression testing after each iteration, with only few changes since the last
test. This strategy also minimised development risk by allowing me to cease
development at the end of any iteration and still have a successful system,
albeit supporting fewer features than planned.

I used Git for version control, with the primary repository hosted on GitHub
which also served as a remote backup of my work. To amplify the benefit of my
regression tests for bug detection, I set up continuous integration such that
each time the GitHub repository was updated, the code was downloaded to a
private VPS, and the full test suite ran automatically. If any tests failed, an
email warning was sent to me, so I was immediately alerted of the problem I had
introduced, and could fix it while the changes which had triggered it were
still fresh in my mind. If all tests passed, the new code was deployed to a web
server, such that a live demo was always accessible online at
\href{http://jstyper.clittle.com}{http://jstyper.clittle.com}.

Within each iteration, I began with the specification, as a kind of
requirements analysis. The specification was written using
Ott~\cite{sewell2007ott}, a tool which I learnt to use as the project
progressed. The iteration would be considered complete when each of the other
components conformed to the specification. For the two development tasks, I
proceeded in a test-driven manner, whereby I would first write tests involving
the new language feature, expecting them to fail. Development then aimed to
make these failing test-cases pass. My collection of tests available for
regression testing (and later for benchmarking) thus grew consistently across
the project life-cycle, and by the end of the project 
\input{|"ls ../../src/tests/custom/results -l | wc -l"}such tests had been produced.

Many developers view theoretical computer science as carrying little practical
benefit for them, when the truth is that concrete implementations of the theory
can bring huge practical benefits. It was thus important to me to integrate
smoothly within the existing JavaScript ecosystem rather than enforce a
non-standard workflow for JavaScript developers, thereby reducing the barriers
to entry and making the benefits of static typing more easily discoverable.
Languages like OCaml, although commonly used for this kind of project within
the academic community, are unlikely to be present on a web developer's
computer. Indeed, this is a problem Chaudhuri mentioned having for Flow, which
has been released as an open source project but suffers from a lack of
contributors because the project's main users are not able to understand or
make meaningful contributions to the type checker's code. Instead, I decided to
write the type inference system in JavaScript, thus ensuring it could run on
any system. This decision opened the door for writing a self-hosting compiler,
which could type-check and compile its own source code. It also potentially
made it possible for type checking and compilation to happen in the browser,
though in practice this is likely only advantageous in fairly unusual
circumstances. Instead, development focussed on running the compiler using
Node.js~\cite{nodejs}, a JavaScript interpreter designed for use outside the browser.

As an interpreted language, JavaScript is not compiled prior to execution, but
it is often compressed to speed up download times to client computers. A number
of static analysis tools exist to aid with this, performing mainly control-flow
analyses to, for example, eliminate unused variables from the source code
(known as \textit{minification}). Although I was not interested in such
analyses, it seemed sensible to integrate with a standard abstract syntax tree
(AST) format, and use existing parsing and code-generation solutions.
UglifyJS~\cite{uglify} is one of the most-used minification libraries for
JavaScript, and has separate components for parsing and code generation but
uses a custom (albeit well-documented) AST format. Esprima~\cite{esprima} is
another JavaScript parser which is marginally faster and uses the SpiderMonkey
AST format standardised by Mozilla. Various analysis tools and code generators
exist for this format, but I decided that the UglifyJS format was actually more
useful for my purposes, and parse speeds was not a priority. For example, the
SpiderMonkey AST has a single representation of any unary expression, when in
practice it can be useful to distinguish between prefix and suffix expressions.
Additionally, UglifyJS uses object inheritance to define its node types, where
Esprima prefers plain objects. 

\chapter{Implementation}\label{implementation}

There were two components to implement for the project -- the formal
specification of the language and the executable type inference system itself.
The language I specified was a subset of JavaScript, chosen to be expressive
enough to represent actual JavaScript programs while being free of some of the
complications of implementation. For example, storage of variables was
simplified to just a scope and a heap, without modelling the special cases of
the global scope, nor worrying about removal of unused data by the garbage
collector. The subset included primitive constants; object and array literals;
arithmetic and logical binary and unary operators; dereferencing from and
assignment to object properties and array elements; functions and function
expressions; and basic control flow via \js{if ... else ...} and loops. 

Where features of JavaScript were omitted, they largely tended to be features
which could be desugared into supported features -- for example \js{switch}
statements can be written as a series of \js{if} statements, and do not present
an interesting problem from a typing perspective. No attempt was made to
include a representation of commonly used JavaScript objects, such as the
\js{RegExp} object or the \textit{Document Object Model}, which is used to
represent the structure of an HTML document. Some features were left out to
reduce the complexity of the project. One such example is the concatenation of
strings using \js{+}, which overloads the use of the \js{+} operator to
represent more than one method signature. This is difficult to accommodate for
without using union types, which carry problems of efficiency. Even where a
feature is missing, however, the gradual typing mechanisms usually allow code
fragments using unavailable features to bypass the type checker, and be used as
if they were dynamic.

\section{Formal Specification}

The specification of my JavaScript subset has two components -- a definition of
the operational semantics, and a definition of the typing judgement used. The
executable type inference system has the job of verifying that the initial
program, as typed by the developer, is well-typed. If the specification is
correctly defined, this property can be proven to be preserved throughout the
execution of the program.

\subsection{Operational Semantics}
The operational semantics takes the form of {\color{red}60} inductive rules
describing all possible transitions from one step of computation to another. A
step of computation is described by a configuration triple of the form
$$\langle m, s, \theta\rangle.$$ $m$ is the program code remaining to be
executed, $s$ represents the current scope (the set of variables accessible at
this program point); and $\theta$ represents the heap, where variable values
are stored.
$$\infer[\textsc{Seq2}]
{\langle \mathtt{m_1; m_2; \dots; m_i}, s, \theta\rangle \rightarrow \langle \mathtt{m_1'; m_2; \dots; m_i}, s, \theta\rangle}
{\langle\mathtt{m_1}, s, \theta\rangle\rightarrow\langle \mathtt{m_1'},s, \theta\rangle}$$

A transition rule is described by a series of premises, which, if true,
indicate that the conclusion must also be true. For example, the rule
\textsc{Seq2} tells us that, if some statement $m_1$ can reduce to $m_1'$ using
the store $(s,\theta)$, then it must also be possible to reduce any sequence of
statements beginning with $m_1$ with the same effects on the store. Most of the
rules follow an intuitive understanding of JavaScript's execution semantics, in
terms of evaluation of subexpression before evaluating the entire expression. A
few points are worth further discussion, however.

\begin{program}[t]
 \begin{minted}[numbersep=-12pt,linenos]{javascript}
	function makeCounter(n) {
	 var i = n;
	 return function() {
		if (i>0) {
		 i--;
		}
		return i;
	 }
	}

	var count = makeCounter(10);
	count(); // i is now 9, but the current scope is unchanged
 \end{minted}
 \caption{A function call with side effects}\label{lst:sideEffects}
\end{program}

\subsubsection*{Assignment}

$$\infer[\textsc{Assign1}]
{
  \langle \mathtt{assignTarget \approx e}, s, \theta \rangle \rightarrow
  \langle \mathtt{assignTarget' \approx e}, s, \theta' \rangle
}
{
  \begin{split}
  	&\mathtt{assignTarget} \not = \mathtt{vRef} \\
  	&\langle \mathtt{assignTarget}, s, \theta \rangle \rightarrow
	  	\langle \mathtt{assignTarget'}, s', \theta'\rangle
  \end{split}
}$$

$$\infer[\textsc{Assign2}]
{
  \langle\mathtt{vRef\approx e},s,\theta\rangle\rightarrow
  	\langle\mathtt{vRef\approx e'},s',\theta'\rangle
}{
	\langle\mathtt{e},s, \theta\rangle\rightarrow
		\langle\mathtt{e'}, s', \theta'\rangle
}$$

$$\infer[\textsc{Assign3}]
{
  \langle \mathtt{vRef = v}, s, \theta \rangle \rightarrow
  	\langle \mathtt{v}, s, \theta\oplus\{\textbf{addr}(vRef, s, \theta) : v\} \rangle
}
{
  (vRef, s, \theta) \in dom(\textbf{addr})
}$$

Legal assignment targets are not limited in the ECMAScript specification by
syntax, but rather by whether or not expressions resolve to a reference. For my
specification, I have simplified this in order to make a decision about whether
an assignment is valid on the basis of syntax alone. I have done this by
specifying that the only valid left hand side of an assignment is a value
reference (\textit{vRef}) or an expression which will reduce down to a value
reference (\textit{assignTarget}). A value reference represents a value stored
on the heap. A variable identifier \js{x}, for example, is a valid value
reference. The value on the heap may be an object or array pointing to other
values, and so \js{x.l} and \js{x[n]} are also value references. Although value
references share a syntactic similarity to dereferencable expressions, they are
distinct and do not reduce down to the value itself. This is important, else we
could end up with assignments of the form \js{5 = 6;} which make little sense.
This simplification has the consequence of deeming some valid JavaScript
assignments illegal, such as the following expression:
\begin{program}[H]
 \begin{minted}{js}
	(function(){ return {}; }).x = 5
 \end{minted}
\end{program}
Although this is a valid JavaScript assignment according to the ECMAScript
specification, it would be difficult to use in our operational semantics, since
the object returned by the function has no identifier, and hence no entry in
either $s$ or $\Gamma$. In contrast, there is a direct relation between a value
reference and a location in the store, defined by the function 
$addr(vRef, s, \theta)$.

\subsubsection*{Function Closures}

$$\infer[\textsc{Func}]
{\langle \mathtt{func}, s, \theta\rangle \rightarrow
\langle\mathtt{[\![func, s]\!],} s, \theta\rangle}{}$$

$$\infer[\textsc{CallAnon}]
{\langle\mathtt{[\![function(x_1, \dots, x_i)\{m\},s']\!](v_1, \dots, v_i)}, s, \theta\rangle\rightarrow
	\langle\mathtt{[\![@body\{m\},s_o',\theta_o]\!]},s, \theta_o\rangle
}{
	\begin{split}
		&a_0, \dots, a_i\textbf{ are fresh} \\
		&s_o' = s' \cup \{\mathbf{this}: a_0, \mathbf{x_1}:a_1, \dots, \mathbf{x_i}: a_i\} \\
		&\theta_0 = \theta \oplus\{\mathbf{a_0}: undefined, \mathbf{a_1}:v_1, \dots, \mathbf{a_i}: v_i\}
	\end{split}
}$$

When a function is called in JavaScript, the scope available for variables is
the one which existed at the point of definition for the function, \textit{not}
the point of use. It is thus insufficient to solely store the function code --
we must also save the current scope when a function definition is encountered,
so that this scope can be restored when the function is called. The rule
\textsc{Func} handles this storage by converting a function expression into a
\textit{function closure}, which contains both the function code and the
relevant scope. The rule \textsc{CallAnon} demonstrates an example of such a
closure at the point of use, being converted to a function body in execution
(denoted by $\mathtt{@body\{m\}}$). Fresh heap locations are chosen for the 
function parameters (including the implicit parameter \js{this}), and these
are referenced within the inner scope.

Values foo denoted within $[\![\enspace]\!]$ will never appear in
the original code as typed by the programmer, but can only arise after a
transition. Because of this, the type checker implementation does not need to
be aware of such constructs -- they are only required for our proof of type
safety.


\subsubsection*{The Heap}

$$\infer[\textsc{CallBody1}]
{\langle \mathtt{[\![@body\{m\}, s', \theta]\!]}, s, \theta\rangle  \rightarrow
\langle \mathtt{[\![@body\{m\}, s'', \theta']\!]}, s, \theta'\rangle }
{ \begin{split}
  & \mathtt{m} \not = \mathtt{return\ v;\ m''} \\
  & \langle\mathtt{m}, s', \theta\rangle\rightarrow\langle \mathtt{m'},s'', \theta\rangle
\end{split}
}$$

Concretely, $s$ is a function from variable identifiers to heap addresses, and
$\theta$ is a function from heap addresses to values -- which may either be
primitive values (such as numbers or function closures), or themselves be
functions from property names or numbers to heap addresses (in the case of
objects and arrays respectively). Many operational semantics do not distinguish
between the scope and the heap, combining both into a single entity normally
known as the store. The separation is needed here, however, because JavaScript
is not a \textit{pure} language. When a function is called, it may have
side-effects which must be recorded, but which should not affect the calling
scope (listing \ref{lst:sideEffects}). The only way to achieve this is to
separate the heap (which is modified by the function call) from the scope
(which is not). This is demonstrated in rule \textsc{CallBody1}, where $\theta$
is updated to $\theta'$, but only the scope $s'$ within the function closure is
modified -- not the containing scope $s$.

\subsection{Type Judgement}
The second component of the specification is the type judgement, which is
divided into typing judgements for expressions and typability judgements for
statements. A typing judgement has the form $$\Gamma\vdash e : T\ |_C\
\Gamma'$$ and a typability judgement has the form $$\Gamma \vdash m\ |_C\
\Gamma'.$$

$\Gamma$ represents the type store, functioning similarly to the store from our
operational semantics by providing a map from variable identifiers to types.
\js{e} is the expression being judged, and $T$ is a valid type for this
expression. Statement \js{m} do not themselves have a type, so a we use a
typability judgement of the same form without $T$. The typabilty judgement
asserts that all sub-expressions of the statement are well-typed. $C$ is a set
of constraints which must be satisfied for the type judgement to be considered
valid. Finally, evaluating the expression may result in changes to the type store (for
example introducing a new variable), and these changes are reflected in
$\Gamma'$, which will be used for judging the type of subsequent expressions.

$$\infer[\textsc{PropType}]{
	\Gamma \vdash \mathtt{e.l}:T\ |_{C\cup\{\{l:T\}\succeq T_1\}}\ \Gamma_2
}{\begin{split}
	&T \textbf{ is fresh} \\
	&\Gamma\vdash\mathtt{e}:T_1\ |_C\ \Gamma_1
  \end{split}}$$
The structure of the inductive rules is the same as for the operational
semantics. For example, rule \textsc{PropType} indicates that \js{e.l} only
has a valid type if \js{e} does, and if the type of \js{e} is a subtype
of the type $\{l: T\}$ for some $T$ (i.e.~if the type of \js{e} has at least the property
\js{l}). Again I will focus on a few interesting cases.

\subsubsection{Return}

$$\infer[\textsc{RetTypable1}]{
	\Gamma \vdash \mathtt{return}\ |_C\ \Gamma'
}{
	\Gamma \vdash \mathtt{return\ undefined}\ |_C\ \Gamma'
}$$
$$\infer[\textsc{RetTypable2}]{
	\Gamma \vdash \mathtt{return\ e}\ |_C\ \Gamma'\cup\{return: T\}
}{
  \begin{split}
	&\Gamma \vdash \mathtt{e}\ |_C\ \Gamma'\\
	&\Gamma'[``return"] = \mathtt{pending}
  \end{split}
}$$
$$\infer[\textsc{RetTypable3}]{
	\Gamma \vdash \mathtt{return\ e}\ |_{C\cup\{T=T'\}}\ \Gamma'\cup\{return: T\}
}{
  \begin{split}
	&\Gamma \vdash \mathtt{e}\ |_C\ \Gamma'\\
	&\Gamma'[``return"] = IllDefined(T')
  \end{split}
}$$
$$\infer[\textsc{RetTypable4}]{
	\Gamma \vdash \mathtt{return\ e}\ |_{C\cup\{T=T'\}}\ \Gamma'
}{
  \begin{split}
	&\Gamma \vdash \mathtt{e}:T\ |_C\ \Gamma'\\
	&\Gamma'[``return"] = T'
  \end{split}
}$$
{\color{red} I also reference rules AnonVoid, NamedVoid, AnonFun, NamedFun,
  IfTypable, WhileTypable and ForTypable in this section. Should I copy those
  rules out here? }

When a function has multiple return statements, we need to ensure that every
possible value returned has the same type.  This parallels the situation with
variable assignments, where we need to ensure that each assignment is of a
suitable type. I thus use the same mechanism for return values as for
variables, and include a special ``\textit{return}" entry in the type
environment. The absence of a return statement in a function indicates a
function which effectively returns the value \js{undefined}.  However we cannot
simply initialise $\Gamma[``return"]$ to the type \texttt{undefined}, since
later return statements are likely to have types which are not equal to
\texttt{undefined}. Instead, $\Gamma[``return"]$ is initialised to a special
type \texttt{pending}, and rule \textsc{RetTypable2} handles the first
encountered return statement by simply overwriting the \texttt{pending} entry
in the type environment with a new one. If $\Gamma[``return"]$ is still
\texttt{pending} after typing the function body, then the function is typed as
having return type \texttt{undefined} (rules \textsc{AnonVoid} and
\textsc{NamedVoid})

Matters are complicated slightly in the presence of non-trivial control flow.
The function below, for example, is not typable because it may sometimes return
a \texttt{number}, but sometimes it will return \texttt{undefined}.
\begin{program}[H]
  \begin{minted}{javascript}
	function isPositive(x) {
		if (x>0) {
			return true;
		}
	}
  \end{minted}
\end{program}

Because there is no explicit \js{return undefined;} statement at the end of the
function, an unsatisfiable constraint is not generated. This problem requires
some primitive control flow analysis to solve. As part of the $merge$ operation
of rules \textsc{If}, \textsc{While} and \textsc{ForTypable}, type environment
entries are given an ``$IllDefined$" wrapper type if they are well-defined in
one branch but not in the other. In particular, this operation will apply to
the \textit{return} entry. If the return type is ill-defined when a new return
statement is encountered, the two types will be constrained, and the
ill-defined one will be overwritten by a well-defined type
(\textsc{RetTypable3}).  If the return type is still ill-defined after typing
the function body, then no type judgement is applicable, and the function is
not typable (rules \textsc{AnonFun} and \textsc{NamedFun} both require that
$\Gamma[``return"]\neq IllDefined(T)$ for any type T).

\subsubsection*{Origin Chains}
It is very common to add properties to an object
after its creation. Some approaches to typing JavaScript make the assumption
that all objects have a larger `potential' type than they are created with, and
uninitialised properties will be added during an extended initialisation
phase~\cite{anderson05}. Analysis of JavaScript's dynamic behaviour in
practice, however, suggests that adding properties is likely to happen at any
point in the object's life cycle~\cite{JSBehaviour}, and so potential types are
not a good model for dynamic objects.

Instead, one must consider two separate object types -- one where the new
property is present, and one where it is not. Listing~\ref{lst:propAdd}
illustrates one situation in which two distinct types are needed. At the start
of the function, \js{x} must have type $$\mathtt{\{b:\{c:\{d:\ number\}\}\}},$$
otherwise line 3 will involve an undefined property access. At the end of the
function, after adding the property \js{d2}, \js{x} should have type $$\mathtt{\{b:\{c:\{d:\ number, d2:\
  boolean\}\}\}}.$$ When determining the type
of \js{f}, we must have access to both of these types in order to construct the
correct function type: $$\mathtt{\{b:\{c:\{d:\ number\}\}\} \rightarrow
  \{b:\{c:\{d:\ number,\ d2:\ boolean\}\}\}}.$$

Let the initial type of \js{x} be $T_x$, and let the type of \js{x} after the
property addition be $T_x'$. It is sadly insufficient to simply replace $T_x$
with $T_x'$ in the type store after the property addition.  The reason for this
is the property access on line 3. This is only a valid operation if \js{x} has
a property \js{b}, which has a property \js{c} with property \js{d}. Indeed the
type judgement for this statement will generate constraints of the form 
\begin{equation}
 \begin{split}
 \label{eq:barC}
 \{b:T_b\} \succeq T_x' \\
 \{c:T_c\} \succeq T_b \\
 \{d:number\} \succeq T_c \\
\end{split}
\end{equation}

The problem is that these only constrain $T_x'$, and not $T_x$. $T_x$ is
left essentially unconstrained, and \js{f} is incorrectly given the type
$$\mathtt{(\mathtt{\{\} \rightarrow \{b:\{c:\{d:\ number,\ d2:\ boolean\}\}\}})}.$$ In order to
correctly judge the type of this function, we need constraint~\eqref{eq:barC}
to pass through $T_x'$ to $T_x$. I do this by attaching $T_x$ as the
\textit{origin} of $T_x'$. I represent this by $T_x'\orig T_x$. When a
non-root object type (i.e.~one which has an origin defined) is constrained as a
subtype, it may either have the required properties itself, or find them
further down the origin chain. Formally,
\begin{equation}
 \begin{split}
	T_1\enspace \succeq\enspace T_2\orig T_2' \iff \exists\ & T_a, T_b\ .\\
	& T_a\cup T_b = T_1 \quad\land \\
	& \forall \left\{l: T\right\} \in T_a \enspace.\enspace (\{l: T'\}\in T_2\enspace \land \enspace T \succeq T') \quad\land \\
	& T_b \succeq T_2'
 \end{split}
 \label{succeq}
\end{equation}
{\color{red}\{Do I need to give some explanation of the formula above? I think
  of it as splitting $T_1$ into two, half of which is present in $T_2$, and
  half of which is present in $T_2'$ (which may itself have a further
  origin)\}}
and
\begin{equation}
 T_1\orig T_1'\enspace \succeq \enspace T_2 \iff T_1\succeq T_2 \land T_1' \succeq T_2
\end{equation}

\begin{program}[t]
 \centering
 \begin{minipage}[b]{0.45\linewidth}
 	\begin{minted}[numbersep=-12pt,linenos]{js}
	function f(x) {
	 x.b.c.d2 = true;
	 x.b.c.d += 1;
	 return x;
	}	
 	\end{minted}
 	\vspace{23mm}
 \end{minipage}
 \quad
 \begin{minipage}[b]{0.45\linewidth}
 	\resizebox{3in}{!}{
 	 \def\svgwidth{200pt}
 	 \import{../res/}{origin.pdf_tex}
 	}
 \end{minipage}
 \caption{Property addition}\label{lst:propAdd}
\end{program}

This system allows the constraint to propagate along the origin chain, finally
enforcing it on the root object. All this is clearly not necessary in the case
of assignment to a pre-existing, rather than novel, property. It is not
possible to determine from the expression alone, however, whether the property
is novel or not, and so my system treats every property assignment as a
property creation. This introduces the possibility of conflict between the type
of the property in $T_x$ and the type of the property in $T_x'$. I resolve this
by adding a special \textit{optional} constraint $\succeq_o$. This verifies
that the novel property is constrained if it already exists, but the constraint
is trivially satisfied if the property did not exist. Formally, the definition
is similar to
\ref{succeq}:
\begin{equation}
 \begin{split}
	T_1\enspace \succeq_o\enspace T_2&\orig T_2' \iff \\
	& \forall \left\{l: T\right\} \in T_1 \enspace.\enspace (\{l: T'\}\in T_2\enspace \implies \enspace T \succeq T') \quad\land \\
	& T_b \succeq_o T_2'
 \end{split}
 \label{succeqo}
\end{equation}
It would be possible to introduce some control-flow analysis to determine which 
properties have been assigned to before, and only attach origin objects if the
assignment is to a novel property. Since most assignments are likely to existing 
properties, this would increase performance of the compiler.

$$\infer[\textsc{PropAssignType}]{
  \Gamma\vdash\mathtt{id.l_1.\cdots.l_k.l = e}:\ T\ |_C\ \Gamma_1
}{
  \begin{split}
	&\Gamma\vdash\mathtt{e}:T'\ |_{C'}\ \Gamma_0 \\
	&\Gamma_0\vdash\mathtt{id}:T_0\ |_{C_0}\ \Gamma_1 \\
	&\Gamma_0\vdash\mathtt{id.l_1}:T_1\ |_{C_1}\ \Gamma_2 \\
	&\vdots \\
	&\Gamma_0\vdash\mathtt{id.l_1.\cdots.l_k}:T_k\ |_{C_k}\ \Gamma_{k+1} \\
	&\Gamma' = \Gamma_{k+1}\cup\{\textbf{id}: \{ \mathbf{l_1}: \{\cdots \{\mathbf{l_k}:\{\mathbf{l}: T'\}\orig T_k \} \cdots\} \orig T_1\}\orig T_0\} \\
	&C = \{T_k \succeq_o \{\mathbf{l}:T'\}\} \cup C' \cup \bigcup_{j=0}^{i}{C_j}
  \end{split}
}$$

The typing rule \textsc{PropAssignType} describes the process of adding a
property for any chain of object value references. Note that a full object is
created, (not simply a small object for the innermost member), and that each
intermediate member needs to be attached to its own origin so that constraints
at other levels are correctly passed through. \textsc{PropAssignType} does not
allow adding properties to value references which include array types, because
adding a property to one object in an array will not change any of the other
objects, and so the type of the array itself cannot be modified in this way.

Origin chains are largely orthogonal to the rest of the typing rules, except
for those involving interesting control flow (i.e.~rules for \js{if},
\js{while} and \js{for}). This is due to the possibility of a branch not being
taken, and so we cannot be certain whether or not a certain property has been
added. Instead, we merge together the different potential type environments,
essentially taking the intersection of the possible types.

\subsubsection{Function Closures}
$$
\infer[\textsc{V\_Closure}]{
	\Gamma\vdash\mathtt{[\![func, s]\!]}: [\![T, \gamma]\!]\ |_C\ \Gamma
}{
	\begin{split}
		&dom(\gamma) \subseteq dom(s) \\
		&\gamma \vdash \mathtt{func} : [\![T, \gamma]\!]\ |_C\ \gamma'
	\end{split}
}$$

The typing rules for function closures are analogous to the transition rules,
capturing the type environment, rather than the actual store, in a closure
type. The restriction of $\gamma$'s domain is necessary to show that \js{func} will be 
well-typed when using $s$ as scope in particular.

$$\infer[\textsc{VoidBodyType}]{
	\Gamma\vdash\mathtt{[\![@body\{m\},s,\theta]\!]}:T\ |_{C\cup\{T=T_k\}}\ \Gamma
}{
	\begin{split}
		&\gamma\cup\{\mathbf{return}:\textrm{pending}\} \vdash \mathtt{m}\ |_C\ \gamma' \\
		&\gamma \vdash (s, \theta) \\
		&\gamma'[``return"] = T_k\\
		&T_k \neq IllDefined(T_k')
	\end{split}
}$$

When using the closure's type environment after a function call, we must first
assert that the captured store is well-typed under this type environment
($\gamma \vdash (s, \theta)$). In essence, this tells us that the type
environment $\gamma$ and the store $(s, \theta)$ agree on the types of
variables. This is necessary for our proof of progress, which will be discussed
further in chapter \ref{evaluation}. Note again that the expression
$\mathtt{[\![@body\{m\},s,\theta]\!]}$ will never actually appear in the
original program code, and so the type judgements for these kinds of expression
are required for the proof of type safety, but do not appear in the executable
type inference system.

\section{Inference Implementation}
The purpose of the implementation is to verify that the original source code is
well-typed according to the type judgements. Once this is verified, I have
proven that the program will not get stuck without a transition available.
There are two parts to verifying this: generating constraints according to the
typing rules defined, and solving these constraints. If the constraints are
satisfiable, then the program is well typed. Although all that is required for
type-safety is this satisfiability, we are also interested in finding an
explicit type for each expression, as an aid for development tools.

\subsection{Constraint Generation}

With the type judgements formally specified, generating constraints is a fairly
straightforward affair.  UglifyJS parses the input code and creates an abstract
syntax tree. Each node in the tree inherits from a base prototype object
representing the node type.  For example, the expression \js{x.l = 5} is
represented by an \textit{AST\_Assign} node containing an \textit{AST\_Dot} and
an \textit{AST\_Number} node.  I added a \js{check(gamma)} method to the
prototype object for each node type. The parameter for the method is the type
environment to use to check the node, and the method returns a judgement object
containing the type (if the node was an expression node), an array of
constraints, and a modified type environment. The inferred type is also
attached to the node, so that future tools can build on the inference
performed. Listing~\ref{lst:arrimpl} shows the \js{check} function
corresponding to the typing rule \textsc{V\_Arr}, which is shown in figure~\ref{fig:arrrule}
\begin{figure}[t]
$$\infer[\textsc{V\_Arr}]{
  \Gamma\vdash\mathtt{[e_1,\, \ldots,\ e_k]}:[T]\ |_C\ \Gamma_k	
}{ 
	\begin{split}
		&T \textrm{ is fresh} \\
		&\Gamma\vdash \mathtt{e_1}:T_1\ |_{C_1}\ \Gamma_1 \\
		&\vdots \\
		&\Gamma_{k-1}\vdash \mathtt{e_k}:T_k\ |_{C_k}\ \Gamma_k \\
		&C=\bigcup_{i=1}^kC_i \cup {T \succeq T_i'}
	\end{split}	  
}$$
\label{fig:arrrule}
\caption{Typing Rule \textsc{V\_Arr}}
\end{figure}
\begin{program}[t]
 \begin{minted}[linenos,numbersep=-12pt]{javascript}
    // Rule V_Arr
    UglifyJS.AST_Array.prototype.check = function(gamma, dynamics) {

      var innerType = gamma.getFreshType();
      var T = new ArrayType({
        innerType: innerType.id
      });
      var C = [];
      // an array's type is constrained by the elements within it
      for (var i = 0; i < this.elements.length; i++) {

        var judgement = this.elements[i].check(gamma);
        C = C.concat(judgement.C);

        C.push(new LEqConstraint(innerType.id, judgement.T.id));

        // thread gamma through to the next element
        gamma = judgement.gamma;
      }
 
      return new Judgement(T, C, gamma);
    };
 \end{minted}
 \caption{The implementation of \textsc{V\_Arr}}\label{lst:arrimpl}
\end{program}


\subsection{Constraint Solution}

Pierce~\cite{pierce} presents a unification algorithm based on Hindley and
Milner's ideas to calculate a solution to a set of equality constraints. The
algorithm is simple and operates in linear time in the number of constraints
(which is itself linear with respect to the program size). It operates by
iteratively turning constraints involving abstract type variables into
substitutions, such that each iteration eliminates one abstract type variable.
This is made possible by the fact that the constraints are equations -- if
$T_1=T_2$, then it is always possible to substitute $T_1$ for $T_2$. If a
constraint is found which equates two different concrete types, the algorithm
fails. A side-effect of this algorithm is finding the \textit{most general
  unifier} for each type variable, which gives a single convenient
representation of the type.  We have generated subtype constraints, however,
which are not so easily solved.

We cannot simply use substitution in the
same way for subtype constraints, because the two constrained types may not be
equal. Even with an algorithm to determine satisfiability, we cannot find a
single most general unifier for object types.  The most precisely we can hope
to pin down a type variable to is within an upper and lower bound.  In
listing~\ref{lst:multSols}, for example, \js{x} must have the property \js{c},
but \js{b} and \js{a} may or may not be present.  {\color{red}Discussion re:
  difficulties, efficiency, etc. Want to make it seem justifiable to cling to
  unification}

\begin{program}
  \begin{minted}[mathescape]{javascript}
		x = {a:1, b:2, c:3} // $x \succeq \{a:number, b:number, c:number\}$
		x.c++;              // $x \preceq \{c:number\}$
  \end{minted}
  \caption{An example of a program with multiple solutions}\label{lst:multSols}
\end{program}

Although these problems are introduced by subtyping constraints, in practice it
is only object types which will encounter them. Primitive types like numbers or
strings, for example, lack any kind of substructure and so a subtype constraint
involving a primitive type can essentially be treated as a straight equation
constraint. Function and array types do have substructure, but solving a
subtype constraint involving these types simply involves passing on the
constraint to a lower level of structure. Table {\color{red}X} summarises these
solution methods for constraints between different kinds of type. Since most
types can be handled easily, it would be nice to find a solution for objects in
terms of substitution, and hence retain the benefits of the linear-time
unification algorithm. 

{\color{red}Table of constraint resolution goes here.}

\subsubsection*{Object Subtype Constraints} 
Let $A$ be an abstract type, and $O$ be a concrete object type. For the
constraint $A \preceq O$ to be satisfied, $A$ must be an object type with at
least all the properties of $O$. If some property was present in $O$ but not in
$A$, then $A$ would not be a valid subtype of $O$. So for this constraint it is
reasonable to use the substitution $[O'/A]$, where $O'$ is a clone of $O$. The
reason we do not use $O$ directly is that later constraints may require $O'$ to
grow, but this should not have an effect on $O$.

For the inverse constraint, $A\succeq O$, the substitution is perhaps not so
obvious. Although the same substitution $[O'/A]$ does satisfy the constraint,
by using it we are rejecting any object types smaller than $O$, which would
also satisfy the constraint. This approximation means that some potentially
legal programs will be rejected by the implementation. Of course, static
type-checking is inherently conservative.  Any type checker which is sound and
decidable must be incomplete -- that is, it will always be possible to write a
program which is well typed but does not satisfy the type checker. The
approximate substitution used here maintains type safety -- any program which
satisfies it must be well typed -- at the cost of a slight reduction in
completeness. Empirical observation through testing fragments of JavaScript has
not revealed any `real' programs which suffer as a result of this
approximation, however.

The last case to consider is a constraint between two object types,$O_1\succeq
O_2$. To simplify discussion, I will not consider object chains or optional
constraints here, though the extension to these cases is apparent from
equations~\ref{succeq}~--~\ref{succeqo}. {\color{red}Is it? It is to me...} We
can check whether the constraint is satisfied by inspection -- iterate through
all properties of $O_1$ and ensure they are present in $O_2$. If it is
satisfied, we can simply move onto the next constraint. If it is not satisfied,
however, we should not yet declare the program untypable. If the constraint was
deemed unsatisfied, $O_1$ must have a property which is absent in $O_2$. Since
the substitution from an abstract variable only represented a bound on the
object, we may be able to `grow' $O_2$ by adding the missing properties.
{\color{red}I don't think this is sound. Firstly, I'm not sure my $\succeq_c$
  constraints always ensure that it is valid to grow $O_2$. Secondly, even if
  it is valid to solve this equation by growing $O_2$, these changes won't be
  constrained by constraints we've already solved and discarded. How do I
  proceed at this point?  I think a correct solution will need to keep track of
  the upper and lower bounds for each object type, and only collapse the bounds
  down to a single object when we need to -- but that's not enough of a
  solution to present here!}

\subsubsection{Subconstraints}
Objects, functions and arrays all have sub-structures, such that once we are
satisfied that two types match at a certain level, we will need to generate
sub-constraints to ensure that the substructures also correspond to one
another. For a constraint $O_1\succeq O_2$ between object types, this is
simple, following on from the definition of $\succeq$. For all $\{l:T_1\}$ in
$O_1$, we must have some $\{l:T_2\}$ in $O_2$. In this, case we simply push the
constraint down, generating the new constraints $T_1\succeq T_2$. Array types
are similar.  For a constraint $[T_1]\succeq[T_2]$, we simply generate the new
constraint $T_1\succeq T_2$. Finally, functions behave in a similar way with
regards to their return type, but are contravariant in their argument types.
Hence for a constraint $(T_1, \ldots, T_i \rightarrow T_r) \succeq(T_1',
\ldots, T_i' \rightarrow T_r')$, we generate the new constraints $T_1\preceq
T_1'$, \dots, $T_i\preceq T_i'$, and $T_r \succeq T_r'$

This generation of subconstraints carries with it the risk of non-termination,
in the case where one type contains itself. Indeed, this is a very real
possibility for JavaScript, since every object method has an implicit \js{this}
parameter which will have the type of the object itself. The solution is to
keep track of which types we have passed as we recursively generate the
subconstraints. If we generate a constraint involving types which we have
already encountered, then it is skipped. The soundness of this can be proved by
induction. A constraint near the top of the path is satisfied if all
subconstraints are satisfied. To prove the subconstraints are satisfied, we can
assume by induction that the constraint itself is satisfied. If a subconstraint
involving the same types appears later on in the path, then we can use this
assumption to immediately show that it is satisfied, and hence we can simply
skip its generation to avoid infinite recursion.

\subsubsection{Functions and Arrays as Objects}

Although I have modelled functions, arrays and objects as all having distinct
types, in practice JavaScript does not make a strong distinction between the
three. In JavaScript, everything which is not a primitive type is an object
type. This means that arrays and functions can have properties added and
removed freely. It could be argued that this is an undesirable behaviour which
a type system should exclude, since for example adding a property to an array
may well be indicative of a bug. However, there are some indispensable
properties of arrays and functions which can only be accessed by dereferencing
them as objects. Chief amongst these is the \js{.length} property of an array,
which is fairly crucial for most tasks involving arrays.

The solution is to treat, as JavaScript does, arrays and functions as if they
were simply objects. We can rewrite array types as object types with a special
`\texttt{@deref}' property representing the array items' type. This then allows
property addition and dereferencing like any other object. Since array items'
types are covariant with the type of the array itself (as object properties
are), we do not need to do anything different for subconstraint generation. A
little more care is required to rewrite function types, since argument types
are covariant, but the principle is the same. Function types are wrapped by an
object type with a special `\texttt{@call}' property, containing the original
function type. Constraints on the object wrapper will generate a covariant
subconstraint on the function type, which can be handled as discussed in the
subconstraints section above.

\section{Gradual Typing}

As previously mentioned, any static type checker must exclude some well typed
programs.  Although this is a problem for any language, it is all the more
likely to arise in a dynamic language like JavaScript, where the programmer is
used to having a certain freedom of expression. A mechanism is thus needed to
give programmers freedom to write dynamic could where convenient, while still
using a static type checker wherever possible. To allow for this, a programmer
can annotate his program with an ``import" directive, which declares a certain
variable as having dynamic behaviour. When read, we cannot rely on an imported
variable to have any particular type, and so we must instead insert a dynamic
type-check to ensure that its type is as what we expect.
Listing~\ref{lst:import1} shows one situation which can only be considered safe
\js{x} is a boolean value when it first appears, but is then a numeric value on
the next line. Although this may seem a contrived example, a very similar
situation could arise if \js{x} was instead used as a function, returning a
different type of value depending on its parameters.
\begin{program}
  \begin{minted}
	// jstyper import x
	if (x) {
		sum = x + 5;
	}
  \end{minted}
  \caption{A simple use of imported variables}\label{lst:import1}
\end{program}

When type-checking a variable identifier, my implementation first checks
whether the variable's name is in the ``imported" list. If it is, the current
program point is recorded, and a fresh type is returned. After type checking is
complete, this fresh type will have been substituted away, and will tell us
what type is required at the program point. The gradual-typing compiler can
then replace all such variables with an appropriate wrapper which will check
that the dynamically check that the variable has the correct type before
returning its value.  This wrapper acts as a kind of explicit cast for the
variable -- from an unknown dynamic type, to some known fixed type. The wrapper
will only return values of the specified type, regardless of the actual type of
the dynamic variable (though it may throw an error instead of returning).
Since the cast is type-preserving, then, the resulting code must be type-safe,
and our guarantee that the program will not get stuck is maintained. We may yet
generate \textit{cast} errors -- if the wrapper is given data of the incorrect type --
but the well-typed portion of the program cannot be blamed for this error,
which must have been caused by the dynamic code returning a value of the
incorrect value. 

\begin{program}
  \begin{minted}
	// jstyper import x
	var sum = (function(t) {
		if (typeof t !== "number") 
			throw new CastError("x is not a number");
		return t;
		})(x) + 5;
  \end{minted}
  \caption{An example primitive wrapper}\label{lst:importPrim}
\end{program}
Although it may appear that we have not gained anything by replacing type
errors with cast errors, it must be remembered that, in JavaScript, an explicit
type error may never be thrown. Instead, values of incorrect types are silently
coerced from one type to another, and the error can propagate quite some
distance through the program before being spotted. The guarantee of a visible
cast error means that this kind of bug is much more likely to be spotted early
in the development process. The additional safety guarantees -- that the
non-dynamic code cannot be blamed -- also restricts the location in which it
may have arisen, which will help locate and correct the bug much faster.

\subsubsection{Higher Order Casts}

The wrapper for a primitive type is simple enough -- simply check the type of
the variable and return it if it is correct. When looking at higher-order
types, we cannot determine by immediate inspection whether the data has the
correct type. Almost by definition, for example, we cannot determine the return
type of a dynamic function by looking at it. Object and array types have the
same problem, since properties can be defined by separate getter and setter
functions, and there is no guarantee that a dynamic getter function will always
return the same type of value. We will only be able to know the return type
once the function has executed, and a return value has been given.

And so, our wrappers for higher order types do exactly that -- they mimic the
original function by accepting arguments, passing them on to the inner
(dynamic) function, then finally examining the return type. If the return type
is itself of a higher order, we once again cannot examine its type directly, so
we instead give it another mimic wrapper before returning it. This wrapping is
similar to the decorator pattern in object-oriented design, where the wrappers
functionality is determined by the inner object. 

Unfortunately, by passing on parameters directly to the inner function, we are
introducing a new potential hole in our type system. The parameters are well
typed, but the inner function is not, so we cannot guarantee that the
parameters are used in a type-safe manner. If the parameter is a callback
expecting an integer, for example, the wrapped inner function could pass it a
string without triggering a cast error. The solution is to guard the parameters
with a different kind of wrapper as they enter the untyped world. The guard
will check the callback's parameters (wrapping them in a mimic if necessary),
call the callback itself, then pass on the return value into the untyped world
(wrapping it in a guard if necessary). 

With this system of guards and wrappers, we control the boundary between the
typed and untyped worlds. Unchecked data is given a mimic to ensure it is safe
in the well-typed world, and type safe data is given a guard to ensure its
safety is not compromised by anything in the unchecked world.

\subsubsection{Performance}

Inserting extra dynamic checks will inevitably have an impact on the
performance of the program. If the program makes frequent use of higher-order
data -- and the common callback paradigm in JavaScript means this will often be
the case -- the memory footprint of the program may be significantly increased
by the wrappers created. An empirical analysis of these effects are included in
Chapter~\ref{evaluation}.

Although this performance cost will be disappointing, there are ways the
problem can be mitigated. It should be noted that there are two main reasons
for a variable to be imported into the type safe world. The first reason is
because the variable is truly dynamic, and the type of the variable can indeed
change at any point. For these, every check is necessary. The second reason to
import a variable, however, is that some section of code is either inaccessible
for static checking (for example if it has been dynamically loaded), or the
type checker has erroneously rejected it (for example if the code uses an
unsupported language feature). In this latter case, inserting a check at every
point of use is unnecessary, since the type of the variable will not change. We
can optimise for performance by distinguishing between these two cases, and in
the latter, only inserting a check at the first use.

Another point to consider is that the JavaScript interpreter must run this kind
of type check at all times anyway. In order to perform a coercion between
types, for example, it must first check to see what types the values are. If
the interpreter is made aware that part of the code has already been statically
verified as type safe, it can avoid making such checks. By protecting the
boundaries of the type checked world, we are guaranteeing that it will be free
of type errors. Interpreters may in fact be able to use this knowledge to
achieve better overall performance than the unchecked code could previously
allow.

\printbibliography{}

\chapter{Evaluation}\label{evaluation}
\section{Proof}
TBC
\section{`Sanity Check': visual inspection of my test cases}
\section{Correctness tests comparing output of code with and without gradual typing}
\section{Speed and Memory Overhead}


\chapter{Conclusions}
\section{Future work}
Prototypal Inheritance, Strings as objects, functions as objects \\
Union types, with discharge under type-checks
Auto-detect imported vars by scope analysis \\
Fine-grained object modification by control flow analysis

\end{document}
